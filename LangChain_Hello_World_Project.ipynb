{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkyuri421/PIAIC_B63_Q1/blob/main/LangChain_Hello_World_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKt3y0Gg4ppT",
        "outputId": "ca8f3401-1c1a-49f8-8c67-e32f23fc1e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTGXulTN6FNh"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from google.colab import userdata\n",
        "api_key=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GyiKgce6Qao"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=api_key,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7  # Adjust for creativity\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfNOtQ-e7TNO"
      },
      "outputs": [],
      "source": [
        "prompt_template= PromptTemplate(\n",
        "    input_variables=[\"questions\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s73d-_9CMP16",
        "outputId": "24cf74a7-1c41-482d-be85-ce4bedcf2263"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-3bd2bce4554b>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain=LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ],
      "source": [
        "chain=LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a09ly4jrMgb1",
        "outputId": "56430873-4bcc-4300-9b16-2937f986393a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-f77897cebd60>:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Hello there! How can I help you today?\n",
            "\n",
            "Answer: LangChain is a framework for developing applications powered by large language models (LLMs).  It's designed to make it easier to build applications that go beyond simply prompting an LLM and getting a response.  Instead, LangChain allows you to orchestrate interactions with LLMs, manage external data sources, and build more complex and robust applications.\n",
            "\n",
            "Here's a breakdown of its key features and capabilities:\n",
            "\n",
            "* **Modular Design:** LangChain is built with modularity in mind.  It provides various \"modules\" that handle specific tasks, allowing you to mix and match components to create customized workflows.  This makes it highly flexible and adaptable to different application needs.\n",
            "\n",
            "* **LLM Integration:**  It seamlessly integrates with various LLMs, including OpenAI, Hugging Face Hub models, and others.  This allows you to choose the LLM best suited for your application's requirements.\n",
            "\n",
            "* **Memory:** LangChain offers different memory mechanisms to maintain context across multiple interactions with the LLM.  This is crucial for building conversational applications or applications that require remembering previous inputs and outputs.  Examples include ConversationBufferMemory and ConversationSummaryMemory.\n",
            "\n",
            "* **Chains:**  These are sequences of calls to LLMs or other utilities.  They allow you to chain together multiple operations, such as querying a database, processing the results, and then feeding the information to an LLM for generating a final response.  This is a core feature of LangChain, enabling complex workflows.\n",
            "\n",
            "* **Indexes:**  LangChain provides tools for indexing and querying various data sources, including documents, PDFs, and databases.  This allows you to easily integrate LLMs with your existing data, enabling tasks like question answering over your own documents.\n",
            "\n",
            "* **Agents:**  These are components that decide which tools to use and in what order to achieve a given goal.  They can autonomously interact with LLMs and other tools to accomplish complex tasks.  This is particularly useful for building more intelligent and autonomous applications.\n",
            "\n",
            "* **Callbacks:** LangChain supports callbacks, allowing you to monitor and log the execution of your applications.  This is useful for debugging, monitoring performance, and understanding the flow of your application.\n",
            "\n",
            "**In short:** LangChain simplifies the process of building sophisticated LLM-powered applications by providing a structured and modular framework.  It handles the complexities of interacting with LLMs, managing external data, and orchestrating complex workflows, enabling developers to focus on the application logic rather than the underlying infrastructure.  It's a powerful tool for anyone looking to build the next generation of AI-powered applications.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question (or type 'exit' to quit): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"Exiting the program. Goodbye!\")\n",
        "        break\n",
        "    response = chain.run({\"question\": question})\n",
        "    print(\"Answer:\", response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/0YFz4yJiGZhWHAkA8X8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}